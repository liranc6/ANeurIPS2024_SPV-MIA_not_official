random_seed: 48

#model_name: gpt2
#target_model: ./ft_llms/gpt2/wikitext/target # <-- merged checkpoint dir
#reference_model: ./ft_llms/gpt2/wikitext/refer # <-- merged checkpoint dir

model_name: tiiuae/falcon-rw-1b
target_model: ./ft_llms/falcon/wikitext/target  # <-- Falcon merged checkpoint dir
reference_model: ./ft_llms/falcon/wikitext/refer  # <-- Falcon merged checkpoint dir


dataset_name: wikitext
dataset_config_name: wikitext-2-raw-v1

cache_path: ./cache
use_dataset_cache: true
packing: true
calibration: true

add_eos_token: false
add_bos_token: false
pad_token_id: null

attack_kind: stat # valid attacks: nn, stat
eval_batch_size: 8 # batch size of the evaluation phase
maximum_samples: 200 # the maximum samples number for member and non-member records.

block_size: 128
validation_split_percentage: 0.1
preprocessing_num_workers: 1

mask_filling_model_name: t5-base
buffer_size: 1
mask_top_p: 1.0
span_length: 2
pct: 0.3 # pct_words_masked
ceil_pct: false

int8: false
half: true

perturbation_number: 1 # the number of different perturbation strength / position; debugging parameter, should be set to 1 in the regular running.
sample_number: 10 # the number of sampling

train_sta_idx: 0
train_end_idx: 2000
eval_sta_idx: 0
eval_end_idx: 500

attack_data_path: cache/wikitext/wikitext-2-raw-v1
load_attack_data: false # whether to load prepared attack data if existing.
