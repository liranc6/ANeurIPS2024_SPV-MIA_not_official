debug: False
debug_config_path: null

# Model configuration
model_name: 'gpt2' # "meta-llama/Llama-2-7b-hf"
dataset_name: "wikitext"
dataset_config_name: "wikitext-2-raw-v1"  # The configuration name of the dataset to use
cache_path: "./cache"
use_dataset_cache: false
refer: false
refer_data_source: null
packing: True
token: null
split_model: false
block_size: 128
preprocessing_num_workers: 1

# PEFT configuration
peft: "lora"
lora_rank: 64
lora_alpha: 16
lora_dropout: 0.1
p_tokens: 20  # The number of virtual tokens for prefix-tuning or p-tuning
p_hidden: 128  # The hidden size of the prompt encoder

# Training configuration
learning_rate: 0.0001
lr_scheduler_type: "linear"
warmup_steps: 0
weight_decay: 0
output_dir: "./ft_llms/checkpoints"
log_steps: 200
eval_steps: 100
save_epochs: 100
epochs: 1
batch_size: 16
gradient_accumulation_steps: 1
gradient_checkpointing: false
trust_remote_code: false

# Dataset indices
train_sta_idx: 0
train_end_idx: 6000
eval_sta_idx: 0
eval_end_idx: 600

# Save configuration
save_limit: null

# Quantization configuration
use_int4: false
use_int8: false
disable_peft: false
disable_flash_attention: True

# Tokenizer configuration
pad_token_id: null
add_eos_token: false
add_bos_token: false
validation_split_percentage: 0.1

# Distributed training configuration
distributed_training:
  use_distributed: True
  num_processes: 0 # Number of processes for distributed training
  strategy: "ddp"  # ['ddp', 'fsdp', 'deepspeed']
  devices: "auto" # Automatically set to the number of GPUs available or specify a number
  fsdp_config:                  # only used if strategy == 'fsdp'
    sharding_strategy: "FULL_SHARD" # ["FULL_SHARD", "SHARD_GRAD_OP", "NO_SHARD"]
    auto_wrap_policy: "transformer" # ["transformer", "size", "none"]
    cpu_offload: false

# Wandb configuration
wandb:
  enable: true
  project_name: "LLM_MIA"
  notes: null  # Will be constructed from model_name, dataset_name, and curr_time_str
  mode: "online"                # ["online", "offline", "disabled"]
  log_model: false
  save_code: true

# Dataset configuration
dataset:
  name: "ftg"  # This maps to your current dataset_name
  config_name: "ftg"  # This maps to your current dataset_config_name
  train:
    start_idx: 0  # This maps to your current train_sta_idx
    end_idx: 6000  # This maps to your current train_end_idx
  eval:
    start_idx: 0  # This maps to your current eval_sta_idx
    end_idx: 600  # This maps to your current eval_end_idx