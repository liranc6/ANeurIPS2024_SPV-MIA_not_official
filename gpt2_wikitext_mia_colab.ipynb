{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c3030bca",
      "metadata": {
        "id": "c3030bca"
      },
      "source": [
        "# Colab Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7dce8f59",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dce8f59",
        "outputId": "cf55f94d-74d7-4610-ea50-0a6defe55e14"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(True, 'NVIDIA A40')"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "torch.cuda.is_available(), torch.cuda.get_device_name(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a1b0fe7e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1b0fe7e",
        "outputId": "f3a48a37-6864-4e50-e332-4a5423537820"
      },
      "outputs": [],
      "source": [
        "# Remove old clone\n",
        "# import os, shutil\n",
        "\n",
        "# repo_name = \"ANeurIPS2024_SPV-MIA\"\n",
        "# if os.path.basename(os.getcwd()) == repo_name:\n",
        "#     %cd ..\n",
        "# if os.path.exists(repo_name):\n",
        "#     print(f\"Removing existing {repo_name}...\")\n",
        "#     shutil.rmtree(repo_name)\n",
        "\n",
        "# Clone fork\n",
        "# !git clone https://github.com/maidesu/ANeurIPS2024_SPV-MIA.git\n",
        "# %cd ANeurIPS2024_SPV-MIA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "X2GeMmzFo6dp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2GeMmzFo6dp",
        "outputId": "eae52082-a711-47fb-d3d4-cee449a6b86c"
      },
      "outputs": [],
      "source": [
        "# !pip install -q -r requirements-colab.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a6c8bdc",
      "metadata": {
        "id": "0a6c8bdc"
      },
      "source": [
        "# Target Model Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "480bd020",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "480bd020",
        "outputId": "dd263dfa-af40-4749-cfe6-47de134f7231"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Script exists: True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/liranc6/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "/home/liranc6/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "full_path='/home/liranc6/W25/adversarial-attacks-on-deep-learning/project/ANeurIPS2024_SPV-MIA_not_official/project'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/liranc6/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/home/liranc6/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "/home/liranc6/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/peft/tuners/lora/model.py:311: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 2359296 || all params: 126799104 || trainable%: 1.8606566809809635\n",
            "Folder './cache/wikitext/wikitext-2-raw-v1' already exists.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using pad_token, but it is not set yet.\n",
            "/home/liranc6/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:166: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
            "  warnings.warn(\n",
            "Map: 100%|██████████| 2000/2000 [00:00<00:00, 8572.24 examples/s]\n",
            "Map: 100%|██████████| 500/500 [00:00<00:00, 7987.20 examples/s]\n",
            "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='301' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 301/2000 00:30 < 02:54, 9.73 it/s, Epoch 0.30/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>4.230400</td>\n",
              "      <td>3.836797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>4.078800</td>\n",
              "      <td>3.765302</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  6/125 00:00 < 00:02, 47.03 it/s]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllms_finetune\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m main_llms_finetune\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Run the fine-tuning with the dictionary arguments\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[43mmain_llms_finetune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Launch fine-tuning test using gpt2 and small wikitext-2-raw-v1 slice\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# !accelerate launch ./ft_llms/llms_finetune.py \\\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# --output_dir ./ft_llms/gpt2/wikitext/target/ \\\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# --train_sta_idx=0 --train_end_idx=2000 --eval_sta_idx=0 --eval_end_idx=500 \\\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# --dataset_config_name wikitext-2-raw-v1\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/W25/adversarial-attacks-on-deep-learning/project/ANeurIPS2024_SPV-MIA_not_official/ft_llms/llms_finetune.py:245\u001b[39m, in \u001b[36mmain_llms_finetune\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    242\u001b[39m logger.info(training_args)\n\u001b[32m    244\u001b[39m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args.disable_peft:\n\u001b[32m    248\u001b[39m     model = model.merge_and_unload()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/transformers/trainer.py:1591\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   1589\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   1590\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1591\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1592\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1593\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1594\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1595\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1596\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/transformers/trainer.py:1984\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   1981\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.epoch = epoch + (step + \u001b[32m1\u001b[39m + steps_skipped) / steps_in_epoch\n\u001b[32m   1982\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_step_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m1984\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1985\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1986\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_substep_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/transformers/trainer.py:2328\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2326\u001b[39m         metrics.update(dataset_metrics)\n\u001b[32m   2327\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2328\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[38;5;28mself\u001b[39m._report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m.state.global_step, metrics)\n\u001b[32m   2331\u001b[39m \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/transformers/trainer.py:3066\u001b[39m, in \u001b[36mTrainer.evaluate\u001b[39m\u001b[34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   3063\u001b[39m start_time = time.time()\n\u001b[32m   3065\u001b[39m eval_loop = \u001b[38;5;28mself\u001b[39m.prediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.use_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.evaluation_loop\n\u001b[32m-> \u001b[39m\u001b[32m3066\u001b[39m output = \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3067\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3068\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEvaluation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3069\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[32m   3070\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[32m   3071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   3072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3073\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3074\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3076\u001b[39m total_batch_size = \u001b[38;5;28mself\u001b[39m.args.eval_batch_size * \u001b[38;5;28mself\u001b[39m.args.world_size\n\u001b[32m   3077\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_jit_compilation_time\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output.metrics:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/transformers/trainer.py:3245\u001b[39m, in \u001b[36mTrainer.evaluation_loop\u001b[39m\u001b[34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   3243\u001b[39m observed_num_examples = \u001b[32m0\u001b[39m\n\u001b[32m   3244\u001b[39m \u001b[38;5;66;03m# Main evaluation loop\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3245\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3246\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Update the observed num examples\u001b[39;49;00m\n\u001b[32m   3247\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobserved_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfind_batch_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3248\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/accelerate/data_loader.py:393\u001b[39m, in \u001b[36mDataLoaderShard.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    391\u001b[39m     \u001b[38;5;66;03m# But we still move it to the device so it is done before `StopIteration` is reached\u001b[39;00m\n\u001b[32m    392\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m         current_batch = \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    394\u001b[39m     next_batch = \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n\u001b[32m    395\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m batch_index >= \u001b[38;5;28mself\u001b[39m.skip_batches:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/accelerate/utils/operations.py:160\u001b[39m, in \u001b[36msend_to_device\u001b[39m\u001b[34m(tensor, device, non_blocking, skip_keys)\u001b[39m\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m skip_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    158\u001b[39m         skip_keys = []\n\u001b[32m    159\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensor)(\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m         \u001b[43m{\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m            \u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m    164\u001b[39m     )\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tensor, \u001b[33m\"\u001b[39m\u001b[33mto\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/accelerate/utils/operations.py:161\u001b[39m, in \u001b[36m<dictcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m skip_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    158\u001b[39m         skip_keys = []\n\u001b[32m    159\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensor)(\n\u001b[32m    160\u001b[39m         {\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m             k: t \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m skip_keys \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensor.items()\n\u001b[32m    163\u001b[39m         }\n\u001b[32m    164\u001b[39m     )\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tensor, \u001b[33m\"\u001b[39m\u001b[33mto\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/accelerate/utils/operations.py:167\u001b[39m, in \u001b[36msend_to_device\u001b[39m\u001b[34m(tensor, device, non_blocking, skip_keys)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tensor, \u001b[33m\"\u001b[39m\u001b[33mto\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    168\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n\u001b[32m    169\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m tensor.to(device)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Get the path to llms_finetune.py\n",
        "finetune_script_path = \"/home/liranc6/W25/adversarial-attacks-on-deep-learning/project/ANeurIPS2024_SPV-MIA_not_official/ft_llms/llms_finetune.py\"\n",
        "\n",
        "# Verify the file exists\n",
        "print(f\"Script exists: {os.path.exists(finetune_script_path)}\")\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "dataset_name = \"wikitext\"\n",
        "dataset_config_name = \"wikitext-2-raw-v1\"\n",
        "# Configure arguments as a dictionary\n",
        "\n",
        "target_args = {\n",
        "    \"output_dir\": \"./ft_llms/{model_name}/{dataset_name}/target/\",\n",
        "    \"block_size\": 128,\n",
        "    \"eval_steps\": 100,\n",
        "    \"save_epochs\": 100,\n",
        "    \"log_steps\": 100,\n",
        "    \"dataset_name\": dataset_name,\n",
        "    \"model_name\": model_name,\n",
        "    \"packing\": True,\n",
        "    \"use_dataset_cache\": True,\n",
        "    \"epochs\": 2,\n",
        "    \"batch_size\": 2,\n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"gradient_accumulation_steps\": 1,\n",
        "    \"train_sta_idx\": 0,\n",
        "    \"train_end_idx\": 2000,\n",
        "    \"eval_sta_idx\": 0,\n",
        "    \"eval_end_idx\": 500,\n",
        "    \"dataset_config_name\": dataset_config_name,\n",
        "    \"disable_flash_attention\": True  # Added to avoid flash attention issues\n",
        "}\n",
        "\n",
        "# Import the function\n",
        "sys.path.append(os.path.dirname(finetune_script_path))\n",
        "from llms_finetune import main_llms_finetune\n",
        "\n",
        "# Run the fine-tuning with the dictionary arguments\n",
        "main_llms_finetune(target_args)\n",
        "\n",
        "# Launch fine-tuning test using gpt2 and small wikitext-2-raw-v1 slice\n",
        "# !accelerate launch ./ft_llms/llms_finetune.py \\\n",
        "# --output_dir ./ft_llms/gpt2/wikitext/target/ \\\n",
        "# --block_size 128 --eval_steps 100 --save_epochs 100 --log_steps 100 \\\n",
        "# -d wikitext -m gpt2 --packing --use_dataset_cache \\\n",
        "# -e 2 -b 2 -lr 1e-4 --gradient_accumulation_steps 1 \\\n",
        "# --train_sta_idx=0 --train_end_idx=2000 --eval_sta_idx=0 --eval_end_idx=500 \\\n",
        "# --dataset_config_name wikitext-2-raw-v1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0vcFz6ac9c-7",
      "metadata": {
        "id": "0vcFz6ac9c-7"
      },
      "source": [
        "# Self-prompt Reference Model Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nu04XTaI9gnm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nu04XTaI9gnm",
        "outputId": "c61ee4c4-c049-4670-cc25-2550957d1de0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Script exists: True\n",
            "Running with arguments: <src.parser.Args object at 0x7f6b30211450>\n",
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/liranc6/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model from: ./ft_llms/gpt2/wikitext/target/checkpoint-2000\n",
            "Pad token id is None, setting to eos token id...\n",
            "Preparing datasets...\n",
            "Folder './cache/wikitext/wikitext-2-raw-v1' already exists.\n",
            "Train dataset size: 16585\n",
            "Generating texts...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/6585 [00:00<?, ?it/s]/home/liranc6/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/transformers/generation/utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "  0%|          | 1/6585 [00:04<8:03:14,  4.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample generated text:  lowest bid. \n",
            " The city of Las Vegas had lobbied hard to be the next city to legalize recreational m...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 2/6585 [00:06<5:04:02,  2.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "  0%|          | 3/6585 [00:07<3:57:14,  2.16s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "  0%|          | 4/6585 [00:08<3:25:13,  1.87s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "  0%|          | 5/6585 [00:10<3:07:26,  1.71s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "  0%|          | 6/6585 [00:11<2:56:49,  1.61s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "  0%|          | 6/6585 [00:12<3:56:04,  2.15s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrefer_data_generate\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run_data_generation\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Run the data generation with the dictionary arguments\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43mrun_data_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrefer_data_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# !accelerate launch ./ft_llms/refer_data_generate.py \\\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# -tm ./ft_llms/gpt2/wikitext/target/checkpoint-2000 \\\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# -m gpt2 -d wikitext \\\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# --dataset_config_name wikitext-2-raw-v1\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/W25/adversarial-attacks-on-deep-learning/project/ANeurIPS2024_SPV-MIA_not_official/ft_llms/refer_data_generate.py:104\u001b[39m, in \u001b[36mrun_data_generation\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m     97\u001b[39m     gen_tokens = model.module.generate(\n\u001b[32m     98\u001b[39m         clipped_ids,\n\u001b[32m     99\u001b[39m         num_beams=\u001b[32m1\u001b[39m,\n\u001b[32m    100\u001b[39m         do_sample=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    101\u001b[39m         max_length=input_ids.size(-\u001b[32m1\u001b[39m),\n\u001b[32m    102\u001b[39m     )\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     gen_tokens = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclipped_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_type == \u001b[33m\"\u001b[39m\u001b[33mllama\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    112\u001b[39m     gen_tokens = gen_tokens[:, \u001b[32m1\u001b[39m:]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/transformers/generation/utils.py:1652\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[39m\n\u001b[32m   1644\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   1645\u001b[39m         input_ids=input_ids,\n\u001b[32m   1646\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   1647\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   1648\u001b[39m         **model_kwargs,\n\u001b[32m   1649\u001b[39m     )\n\u001b[32m   1651\u001b[39m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1652\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1653\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1656\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1657\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1658\u001b[39m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1659\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1660\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1661\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1662\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1663\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1664\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1666\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode == GenerationMode.BEAM_SEARCH:\n\u001b[32m   1667\u001b[39m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[32m   1668\u001b[39m     beam_scorer = BeamSearchScorer(\n\u001b[32m   1669\u001b[39m         batch_size=batch_size,\n\u001b[32m   1670\u001b[39m         num_beams=generation_config.num_beams,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1675\u001b[39m         max_length=generation_config.max_length,\n\u001b[32m   1676\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/transformers/generation/utils.py:2734\u001b[39m, in \u001b[36mGenerationMixin.sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2731\u001b[39m model_inputs = \u001b[38;5;28mself\u001b[39m.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\u001b[32m   2733\u001b[39m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2734\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   2735\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2736\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2737\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2738\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2739\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2741\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[32m   2742\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/accelerate/utils/operations.py:636\u001b[39m, in \u001b[36mconvert_outputs_to_fp32.<locals>.forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    635\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m636\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/accelerate/utils/operations.py:624\u001b[39m, in \u001b[36mConvertOutputsToFp32.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m624\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/torch/amp/autocast_mode.py:44\u001b[39m, in \u001b[36mautocast_decorator.<locals>.decorate_autocast\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_autocast\u001b[39m(*args, **kwargs):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1076\u001b[39m, in \u001b[36mGPT2LMHeadModel.forward\u001b[39m\u001b[34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1068\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1069\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[32m   1070\u001b[39m \u001b[33;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[32m   1071\u001b[39m \u001b[33;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[32m   1072\u001b[39m \u001b[33;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[32m   1073\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1074\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1076\u001b[39m transformer_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1078\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1080\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1082\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1083\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1084\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1085\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1086\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1087\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1088\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1089\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1090\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1091\u001b[39m hidden_states = transformer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1093\u001b[39m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:900\u001b[39m, in \u001b[36mGPT2Model.forward\u001b[39m\u001b[34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    890\u001b[39m     outputs = torch.utils.checkpoint.checkpoint(\n\u001b[32m    891\u001b[39m         create_custom_forward(block),\n\u001b[32m    892\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    897\u001b[39m         encoder_attention_mask,\n\u001b[32m    898\u001b[39m     )\n\u001b[32m    899\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m900\u001b[39m     outputs = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    901\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    903\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    904\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    906\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    907\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    911\u001b[39m hidden_states = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    912\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:426\u001b[39m, in \u001b[36mGPT2Block.forward\u001b[39m\u001b[34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[39m\n\u001b[32m    423\u001b[39m     outputs = outputs + cross_attn_outputs[\u001b[32m2\u001b[39m:]  \u001b[38;5;66;03m# add cross attentions if we output attention weights\u001b[39;00m\n\u001b[32m    425\u001b[39m residual = hidden_states\n\u001b[32m--> \u001b[39m\u001b[32m426\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mln_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    427\u001b[39m feed_forward_hidden_states = \u001b[38;5;28mself\u001b[39m.mlp(hidden_states)\n\u001b[32m    428\u001b[39m \u001b[38;5;66;03m# residual connection\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217\u001b[39m, in \u001b[36mLayerNorm.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/torch/nn/functional.py:2900\u001b[39m, in \u001b[36mlayer_norm\u001b[39m\u001b[34m(input, normalized_shape, weight, bias, eps)\u001b[39m\n\u001b[32m   2890\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[32m   2891\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m   2892\u001b[39m         layer_norm,\n\u001b[32m   2893\u001b[39m         (\u001b[38;5;28minput\u001b[39m, weight, bias),\n\u001b[32m   (...)\u001b[39m\u001b[32m   2898\u001b[39m         eps=eps,\n\u001b[32m   2899\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2900\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2901\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackends\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcudnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43menabled\u001b[49m\n\u001b[32m   2902\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Get the path to refer_data_generate.py\n",
        "refer_data_script_path = \"/home/liranc6/W25/adversarial-attacks-on-deep-learning/project/ANeurIPS2024_SPV-MIA_not_official/ft_llms/refer_data_generate.py\"\n",
        "\n",
        "# Verify the file exists\n",
        "print(f\"Script exists: {os.path.exists(refer_data_script_path)}\")\n",
        "\n",
        "# Configure arguments as a dictionary\n",
        "refer_data_args = {\n",
        "    \"model_name\": model_name,\n",
        "    \"target_model\": \"./ft_llms/{model_name}/{dataset_name}/target\",\n",
        "    \"dataset_name\": dataset_name,\n",
        "    \"dataset_config_name\": dataset_config_name,\n",
        "    \"cache_path\": \"./cache\",\n",
        "    \"use_dataset_cache\": True,\n",
        "    \"packing\": True,\n",
        "    \"block_size\": 128,\n",
        "    \"preprocessing_num_workers\": 1,\n",
        "    \"validation_split_percentage\": 0.1,\n",
        "    \"local_files_only\": False\n",
        "}\n",
        "\n",
        "# Import the function\n",
        "sys.path.append(os.path.dirname(refer_data_script_path))\n",
        "from refer_data_generate import run_data_generation\n",
        "\n",
        "# Run the data generation with the dictionary arguments\n",
        "run_data_generation(refer_data_args)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# !accelerate launch ./ft_llms/refer_data_generate.py \\\n",
        "# -tm ./ft_llms/gpt2/wikitext/target/checkpoint-2000 \\\n",
        "# -m gpt2 -d wikitext \\\n",
        "# --dataset_config_name wikitext-2-raw-v1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e888687",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 10604\n",
            "drwxr-xr-x 5 liranc6 domain^users    4096 May 13 19:31 .\n",
            "drwxr-xr-x 3 liranc6 domain^users    4096 May 13 17:21 ..\n",
            "drwxr-xr-x 3 liranc6 domain^users    4096 May 13 18:42 attack_data_gpt2@wikitext\n",
            "drwxr-xr-x 3 liranc6 domain^users    4096 May 13 18:13 refer@gpt2\n",
            "drwxr-xr-x 3 liranc6 domain^users    4096 May 13 19:31 refer@tiiuae\n",
            "-rw-r--r-- 1 liranc6 domain^users 9690328 May 13 17:21 train_dataset\n",
            "-rw-r--r-- 1 liranc6 domain^users 1093224 May 13 17:22 valid_dataset\n"
          ]
        }
      ],
      "source": [
        "# Check if the reference data directory exists\n",
        "# !ls -la ./cache/wikitext/wikitext-2-raw-v1/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb2a2c70",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Script exists: True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/liranc6/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/peft/tuners/lora/model.py:311: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 2359296 || all params: 126799104 || trainable%: 1.8606566809809635\n",
            "Folder './cache/wikitext/wikitext-2-raw-v1' already exists.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using pad_token, but it is not set yet.\n",
            "/home/liranc6/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:166: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
            "  warnings.warn(\n",
            "Map: 100%|██████████| 32/32 [00:00<00:00, 1846.97 examples/s]\n",
            "Map: 100%|██████████| 500/500 [00:00<00:00, 8169.69 examples/s]\n",
            "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [32/32 00:02, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# Get the path to llms_finetune.py\n",
        "finetune_script_path = \"/home/liranc6/W25/adversarial-attacks-on-deep-learning/project/ANeurIPS2024_SPV-MIA_not_official/ft_llms/llms_finetune.py\"\n",
        "\n",
        "# Verify the file exists\n",
        "print(f\"Script exists: {os.path.exists(finetune_script_path)}\")\n",
        "\n",
        "# Configure reference model arguments as a dictionary\n",
        "reference_args = {\n",
        "    \"refer\": True,\n",
        "    \"output_dir\": f\"./ft_llms/{model_name}/{dataset_name}/refer/\",\n",
        "    \"block_size\": 128,\n",
        "    \"eval_steps\": 100,\n",
        "    \"save_epochs\": 100,\n",
        "    \"log_steps\": 100,\n",
        "    \"dataset_name\": dataset_name,\n",
        "    \"model_name\": model_name,\n",
        "    \"packing\": True,\n",
        "    \"use_dataset_cache\": True,\n",
        "    \"epochs\": 2,\n",
        "    \"batch_size\": 2,\n",
        "    \"learning_rate\": 5e-5,\n",
        "    \"gradient_accumulation_steps\": 1,\n",
        "    \"train_sta_idx\": 0,\n",
        "    \"train_end_idx\": 2000,\n",
        "    \"eval_sta_idx\": 0,\n",
        "    \"eval_end_idx\": 500,\n",
        "    \"dataset_config_name\": dataset_config_name,\n",
        "    \"disable_flash_attention\": True  # Added to avoid flash attention issues\n",
        "}\n",
        "\n",
        "# Import the function if not already imported\n",
        "if 'main_llms_finetune' not in globals():\n",
        "    sys.path.append(os.path.dirname(finetune_script_path))\n",
        "    from llms_finetune import main_llms_finetune\n",
        "\n",
        "# Run reference model fine-tuning\n",
        "main_llms_finetune(reference_args)\n",
        "\n",
        "# !accelerate launch ./ft_llms/llms_finetune.py --refer \\\n",
        "# --output_dir ./ft_llms/gpt2/wikitext/refer/ \\\n",
        "# --block_size 128 --eval_steps 100 --save_epochs 100 --log_steps 100 \\\n",
        "# -d wikitext -m gpt2 --packing --use_dataset_cache \\\n",
        "# -e 2 -b 2 -lr 5e-5 --gradient_accumulation_steps 1 \\\n",
        "# --train_sta_idx=0 --train_end_idx=2000 --eval_sta_idx=0 --eval_end_idx=500 \\\n",
        "# --dataset_config_name wikitext-2-raw-v1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pT9KrvIT-ZhF",
      "metadata": {
        "id": "pT9KrvIT-ZhF"
      },
      "source": [
        "# Run SPV-MIA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48e4b40a",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "[nltk_data] Downloading package wordnet to /home/liranc6/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /home/liranc6/nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment ready for attack script\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Add this to a cell in your notebook:\n",
        "# import os\n",
        "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Avoid huggingface tokenizer warnings\n",
        "\n",
        "# Install a compatible version of NLTK for Python 3.11\n",
        "# !pip install -q nltk==3.8.1\n",
        "\n",
        "# Download necessary NLTK data\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "print(\"Environment ready for attack script\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "zG96TTKK-ibT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zG96TTKK-ibT",
        "outputId": "a2b14dfc-3022-4f37-940f-5eb8c1fc66c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Script exists: True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/liranc6/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "/home/liranc6/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "/home/liranc6/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "full_path='/home/liranc6/W25/adversarial-attacks-on-deep-learning/project/ANeurIPS2024_SPV-MIA_not_official/project'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/liranc6/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/home/liranc6/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/transformers/modeling_utils.py:484: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(checkpoint_file, map_location=map_location)\n",
            "05/14/2025 19:22:03 - INFO - run_attack - Successfully load models\n",
            "05/14/2025 19:22:04 - INFO - run_attack - Pad token id is None, setting to eos token id...\n",
            "05/14/2025 19:22:15 - INFO - run_attack - Successfully load datasets!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Folder './cache/wikitext/wikitext-2-raw-v1' already exists.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "05/14/2025 19:22:38 - INFO - attack.attack_model - Preparing data...\n",
            "05/14/2025 19:22:38 - INFO - attack.attack_model - Generating feature vectors for member data...\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/home/liranc6/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING: 1 texts have no fills. Trying again [attempt 1].\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [04:12<00:00, 252.77s/it]\n",
            "05/14/2025 19:26:51 - INFO - attack.attack_model - Generating feature vectors for non-member data...\n",
            "  0%|          | 0/1 [00:59<?, ?it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     18\u001b[39m attack_args = {\n\u001b[32m     19\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel_name\u001b[39m\u001b[33m\"\u001b[39m: model_name,\n\u001b[32m     20\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdataset_name\u001b[39m\u001b[33m\"\u001b[39m: dataset_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreference_model\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m./ft_llms/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/refer/\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     24\u001b[39m     }\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Run the attack with dictionary arguments\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43mrun_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattack_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# # Import directly from the module file \u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# from attack import run_attack as run_attack_func  # Rename to avoid potential conflicts\u001b[39;00m\n\u001b[32m     31\u001b[39m \n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# run_attack_func()\u001b[39;00m\n\u001b[32m     33\u001b[39m \n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# !python attack.py\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/W25/adversarial-attacks-on-deep-learning/project/ANeurIPS2024_SPV-MIA_not_official/run_attack.py:156\u001b[39m, in \u001b[36mrun_attack\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    147\u001b[39m datasets = {\n\u001b[32m    148\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m    149\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m: train_dataloader,\n\u001b[32m    150\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mvalid\u001b[39m\u001b[33m\"\u001b[39m: eval_dataloader\n\u001b[32m    151\u001b[39m     }\n\u001b[32m    152\u001b[39m }\n\u001b[32m    155\u001b[39m attack_model = AttackModel(target_model, tokenizer, datasets, reference_model, shadow_model, cfg, mask_model=mask_model, mask_tokenizer=mask_tokenizer)\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m \u001b[43mattack_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconduct_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/W25/adversarial-attacks-on-deep-learning/project/ANeurIPS2024_SPV-MIA_not_official/attack/attack_model.py:197\u001b[39m, in \u001b[36mAttackModel.conduct_attack\u001b[39m\u001b[34m(self, cfg)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconduct_attack\u001b[39m(\u001b[38;5;28mself\u001b[39m, cfg):\n\u001b[32m    194\u001b[39m     save_path = os.path.join(PATH, cfg[\u001b[33m\"\u001b[39m\u001b[33mattack_data_path\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mattack_data_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg[\u001b[33m'\u001b[39m\u001b[33mmodel_name\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg[\u001b[33m'\u001b[39m\u001b[33mdataset_name\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    195\u001b[39m                              \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mroc_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg[\u001b[33m'\u001b[39m\u001b[33mattack_kind\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.npz\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m     raw_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata_prepare\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtarget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m     feat, ground_truth = \u001b[38;5;28mself\u001b[39m.feat_prepare(raw_info, cfg)\n\u001b[32m    199\u001b[39m     \u001b[38;5;66;03m# self.distinguishability_plot(raw_info['mem_feat']['ori_losses'].mean(-1),\u001b[39;00m\n\u001b[32m    200\u001b[39m     \u001b[38;5;66;03m#                              raw_info['nonmem_feat']['ori_losses'].mean(-1))\u001b[39;00m\n\u001b[32m    201\u001b[39m     \u001b[38;5;66;03m# self.distinguishability_plot(feat[:1000], feat[-1000:])\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/W25/adversarial-attacks-on-deep-learning/project/ANeurIPS2024_SPV-MIA_not_official/attack/attack_model.py:145\u001b[39m, in \u001b[36mAttackModel.data_prepare\u001b[39m\u001b[34m(self, kind, cfg)\u001b[39m\n\u001b[32m    142\u001b[39m         utils.save_dict_to_npz(ref_mem_feat, ref_mem_path)\n\u001b[32m    144\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mGenerating feature vectors for non-member data...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m nonmem_feat, ref_nonmem_feat = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meval_perturb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnonmem_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m accelerator.is_main_process:\n\u001b[32m    147\u001b[39m     utils.save_dict_to_npz(nonmem_feat, nonmem_path)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/W25/adversarial-attacks-on-deep-learning/project/ANeurIPS2024_SPV-MIA_not_official/attack/attack_model.py:95\u001b[39m, in \u001b[36mAttackModel.eval_perturb\u001b[39m\u001b[34m(self, model, dataset, cfg)\u001b[39m\n\u001b[32m     93\u001b[39m sampled_ref_per_losses = []\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(cfg[\u001b[33m\"\u001b[39m\u001b[33msample_number\u001b[39m\u001b[33m\"\u001b[39m]):\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     per_loss, ref_per_loss, per_token_len = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mori_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperturb_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mperturb_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefer_model\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreference_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     sampled_per_losses.append(per_loss)\n\u001b[32m     97\u001b[39m     sampled_ref_per_losses.append(ref_per_loss)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/W25/adversarial-attacks-on-deep-learning/project/ANeurIPS2024_SPV-MIA_not_official/attack/attack_model.py:60\u001b[39m, in \u001b[36mAttackModel.llm_eval\u001b[39m\u001b[34m(self, model, data_loader, cfg, idx_rate, perturb_fn, refer_model)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     59\u001b[39m     outputs = model(**token_ids, labels=labels)\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     ref_outputs = \u001b[43mrefer_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m loss = outputs.loss\n\u001b[32m     62\u001b[39m ref_loss = ref_outputs.loss\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/accelerate/hooks.py:165\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    163\u001b[39m         output = old_forward(*args, **kwargs)\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m     output = \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1076\u001b[39m, in \u001b[36mGPT2LMHeadModel.forward\u001b[39m\u001b[34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1068\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1069\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[32m   1070\u001b[39m \u001b[33;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[32m   1071\u001b[39m \u001b[33;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[32m   1072\u001b[39m \u001b[33;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[32m   1073\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1074\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1076\u001b[39m transformer_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1078\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1080\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1082\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1083\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1084\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1085\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1086\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1087\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1088\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1089\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1090\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1091\u001b[39m hidden_states = transformer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1093\u001b[39m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/accelerate/hooks.py:165\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    163\u001b[39m         output = old_forward(*args, **kwargs)\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m     output = \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:900\u001b[39m, in \u001b[36mGPT2Model.forward\u001b[39m\u001b[34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    890\u001b[39m     outputs = torch.utils.checkpoint.checkpoint(\n\u001b[32m    891\u001b[39m         create_custom_forward(block),\n\u001b[32m    892\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    897\u001b[39m         encoder_attention_mask,\n\u001b[32m    898\u001b[39m     )\n\u001b[32m    899\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m900\u001b[39m     outputs = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    901\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    903\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    904\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    906\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    907\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    911\u001b[39m hidden_states = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    912\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/accelerate/hooks.py:165\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    163\u001b[39m         output = old_forward(*args, **kwargs)\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m     output = \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:390\u001b[39m, in \u001b[36mGPT2Block.forward\u001b[39m\u001b[34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[39m\n\u001b[32m    388\u001b[39m residual = hidden_states\n\u001b[32m    389\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.ln_1(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m attn_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    398\u001b[39m attn_output = attn_outputs[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[32m    399\u001b[39m outputs = attn_outputs[\u001b[32m1\u001b[39m:]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/accelerate/hooks.py:165\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    163\u001b[39m         output = old_forward(*args, **kwargs)\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m     output = \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:312\u001b[39m, in \u001b[36mGPT2Attention.forward\u001b[39m\u001b[34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[39m\n\u001b[32m    310\u001b[39m     attention_mask = encoder_attention_mask\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m     query, key, value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mc_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m.split(\u001b[38;5;28mself\u001b[39m.split_size, dim=\u001b[32m2\u001b[39m)\n\u001b[32m    314\u001b[39m query = \u001b[38;5;28mself\u001b[39m._split_heads(query, \u001b[38;5;28mself\u001b[39m.num_heads, \u001b[38;5;28mself\u001b[39m.head_dim)\n\u001b[32m    315\u001b[39m key = \u001b[38;5;28mself\u001b[39m._split_heads(key, \u001b[38;5;28mself\u001b[39m.num_heads, \u001b[38;5;28mself\u001b[39m.head_dim)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/accelerate/hooks.py:165\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    163\u001b[39m         output = old_forward(*args, **kwargs)\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m     output = \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:990\u001b[39m, in \u001b[36mLinear8bitLt.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bias.dtype != x.dtype:\n\u001b[32m    988\u001b[39m     \u001b[38;5;28mself\u001b[39m.bias.data = \u001b[38;5;28mself\u001b[39m.bias.data.to(x.dtype)\n\u001b[32m--> \u001b[39m\u001b[32m990\u001b[39m out = \u001b[43mbnb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    992\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.has_fp16_weights \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.CB \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    993\u001b[39m     \u001b[38;5;28mself\u001b[39m.weight.data = \u001b[38;5;28mself\u001b[39m.state.CB\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:509\u001b[39m, in \u001b[36mmatmul\u001b[39m\u001b[34m(A, B, out, state, threshold, bias)\u001b[39m\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m threshold > \u001b[32m0.0\u001b[39m:\n\u001b[32m    508\u001b[39m     state.threshold = threshold\n\u001b[32m--> \u001b[39m\u001b[32m509\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMatMul8bitLt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/torch/autograd/function.py:575\u001b[39m, in \u001b[36mFunction.apply\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch._C._are_functorch_transforms_active():\n\u001b[32m    573\u001b[39m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[32m    574\u001b[39m     args = _functorch.utils.unwrap_dead_wrappers(args)\n\u001b[32m--> \u001b[39m\u001b[32m575\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[32m    578\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    579\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    580\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    581\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstaticmethod. For more details, please see \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    582\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    583\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spv_attack311/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:345\u001b[39m, in \u001b[36mMatMul8bitLt.forward\u001b[39m\u001b[34m(ctx, A, B, out, bias, state)\u001b[39m\n\u001b[32m    341\u001b[39m         state.CB, state.SCB, _ = F.int8_vectorwise_quant(B.to(torch.float16))\n\u001b[32m    343\u001b[39m \u001b[38;5;66;03m# Handle sparse decomposition. In some instances, we may have not found any\u001b[39;00m\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# outlier columns at all. In that case, we'll skip this part completely.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m state.threshold > \u001b[32m0.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m outlier_cols \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[43moutlier_cols\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    346\u001b[39m     state.idx = outlier_cols\n\u001b[32m    348\u001b[39m     \u001b[38;5;66;03m# Zero out the outliers in the transposed 8bit inputs.\u001b[39;00m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Get the path to attack.py\n",
        "attack_script_path = \"/home/liranc6/W25/adversarial-attacks-on-deep-learning/project/ANeurIPS2024_SPV-MIA_not_official/run_attack.py\"\n",
        "\n",
        "# Verify the file exists\n",
        "print(f\"Script exists: {os.path.exists(attack_script_path)}\")\n",
        "\n",
        "# Import the attack function\n",
        "sys.path.append(os.path.dirname(attack_script_path))\n",
        "from run_attack import run_attack  # Adjust if the function has a different name\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "dataset_name = \"wikitext\"\n",
        "dataset_config_name = \"wikitext-2-raw-v1\"\n",
        "\n",
        "attack_args = {\n",
        "    \"model_name\": model_name,\n",
        "    \"dataset_name\": dataset_name,\n",
        "    \"dataset_config_name\": dataset_config_name,\n",
        "    \"target_model\": f\"./ft_llms/{model_name}/{dataset_name}/target/\",\n",
        "    \"reference_model\": f\"./ft_llms/{model_name}/{dataset_name}/refer/\",\n",
        "    }\n",
        "\n",
        "# Run the attack with dictionary arguments\n",
        "run_attack(attack_args)\n",
        "\n",
        "# # Import directly from the module file \n",
        "# from attack import run_attack as run_attack_func  # Rename to avoid potential conflicts\n",
        "\n",
        "# run_attack_func()\n",
        "\n",
        "# !python attack.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZwGsi1ORdKk8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "id": "ZwGsi1ORdKk8",
        "outputId": "971fa3c7-2170-47a7-ec80-e28427238a74"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load ROC curve data\n",
        "data = np.load(\"./cache/wikitext/wikitext-2-raw-v1/attack_data_gpt2@wikitext/roc_stat.npz\")\n",
        "fpr = data[\"fpr\"]\n",
        "tpr = data[\"tpr\"]\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {np.round(np.trapz(tpr, fpr), 4)})')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label=\"Random guess\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve – Statistical Membership Inference Attack\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Mark TPR@1%FPR\n",
        "fpr_1_index = np.argmin(np.abs(fpr - 0.01))\n",
        "plt.scatter(fpr[fpr_1_index], tpr[fpr_1_index], color='red', label=f'TPR@1%FPR: {tpr[fpr_1_index]:.3f}')\n",
        "plt.legend()\n",
        "\n",
        "# Save the figure\n",
        "plt.savefig(\"roc_stat.png\", dpi=300)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "spv_attack311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
